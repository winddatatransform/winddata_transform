{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fb773b",
   "metadata": {},
   "source": [
    "## Data Transformation Task\n",
    "<br><br>\n",
    "The windfarm ANH01 has turbines (`wtgid`) ANH01A01, ANH01A02, ...,  ANH01C03.\n",
    "<br><br>\n",
    "Every ten minutes a csv file is generated with columns: `id`, `timestamp`, `wtgid`, `siteid`, `tditag`, `value`, `insert_time`\n",
    "See f.x. \"ANH01_2020-02-20-13-10-00.csv\"\n",
    "<br><br>\n",
    "Different sensors (*tditag*: Tag1, ..., Tag6) produce values of different types (string, integer, datetime, float).\n",
    "<br><br>\n",
    "The data scientists in the operations department require easy access to the time series produced by the sensors `Tag3`, `Tag4` and `Tag5`.\n",
    "<br><br>\n",
    "You thus have to \n",
    "1. filter  \n",
    "2. transform/transpose the data to have columns: `timestamp`, `siteid`, `wtgid`, `Tag3`, `Tag4`, `Tag5`\n",
    "\n",
    "where the tag columns contain the corresponding value with an appropriate datatype.\n",
    "<br><br>\n",
    "Use pyspark, pandas or pure python.\n",
    "There are some classes that you can use to read the files into a pyspark or pandas dataframe.\n",
    "<br><br>\n",
    "Your task is to write the transform method in `TransformJob`.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380eb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as PandasDataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame as PysparkDataFrame\n",
    "\n",
    "\n",
    "class InOutBase(ABC):\n",
    "    \"\"\"Abstract base class for read and write functionality.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def read(self, path: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def write(self, data, path: str):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PandasIO(InOutBase):\n",
    "    \"\"\"Reads and write csv files from/to pandas Dataframes.\"\"\"\n",
    "\n",
    "    def read(self, path) -> PandasDataFrame:\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    def write(self, data: PandasDataFrame, path: str):\n",
    "        data.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "class SparkIO(InOutBase):\n",
    "    \"\"\"Reads and write csv files from/to pyspark Dataframes.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        super().__init__()\n",
    "\n",
    "    def read(self, path) -> PysparkDataFrame:\n",
    "        return self.spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "\n",
    "    def write(self, data: PysparkDataFrame, path: str):\n",
    "        data.coalesce(1).write.mode(\"overwrite\").format(\"csv\").save(path, header=\"true\")\n",
    "\n",
    "\n",
    "class NamedTupleIO(InOutBase):\n",
    "    \"\"\"Reads and write csv files from/to named tuples, \n",
    "       where the header in the csv files corresponds to the field names.\"\"\"\n",
    "\n",
    "    def read(self, path) -> List[namedtuple]:\n",
    "        with open(path, \"r\") as f:\n",
    "            data = []\n",
    "            for idx, l in enumerate(f):\n",
    "                if idx == 0:\n",
    "                    named_tuple = namedtuple(\"row\", l)\n",
    "                else:\n",
    "                    l = l.rstrip()\n",
    "                    if len(named_tuple._fields) == len(l.split(\",\")):\n",
    "                        data.append(named_tuple(*l.split(\",\")))\n",
    "        return data\n",
    "\n",
    "    def write(self, path, data: List[namedtuple]):\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\",\".join(data[0]._fields) + \"\\n\")\n",
    "            for row in data:\n",
    "                f.write(\",\".join(row) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62a5af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformJob:\n",
    "    def __init__(self, file_handler: InOutBase):\n",
    "        self._file_handler = file_handler\n",
    "\n",
    "    def read(self, path):\n",
    "        return self._file_handler.read(path)\n",
    "\n",
    "    def write(self, path, data):\n",
    "        self._file_handler.write(data, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(raw_data):\n",
    "        \"\"\"Finish me!\"\"\"\n",
    "        transformed_data = raw_data\n",
    "        return transformed_data\n",
    "\n",
    "    def execute(self, input_path, output_path):\n",
    "        input_data = self.read(input_path)\n",
    "        output_data = self.transform(input_data)\n",
    "        if not output_path:\n",
    "            return output_data\n",
    "        self.write(output_path, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5cf503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'ANH01_2020-02-20-13-40-00.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c020870",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class InOutBase with abstract methods read, write",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_159742/3885489899.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInOutBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class InOutBase with abstract methods read, write"
     ]
    }
   ],
   "source": [
    "df = TransformJob(InOutBase()).execute(input_path=input_path, output_path=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
